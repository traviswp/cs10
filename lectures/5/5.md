---
layout: page
title: "Lecture NUMBER, MONTH DAY: TOPIC"
---

## Outline
{:.no_toc}

* Outline
{:toc}
# January 14: Video Processing

<a href="../4/4.html">[prev]</a>
<a href="../6/6.html">[next]</a> 

Processing video brings together the concepts we've seen so far. Video is just a 
stream of images, and we know how to represent and manipulate them. And we know 
how to set up a timer to handle the fact that the images arrive periodically, 
though we'll bury that part under the hood here. So we'll look at a couple of 
examples of just doing image processing periodically. But then we'll also consider 
what we can do once we start recognizing that the images are related to each other.

## Outline ##

* <a href="#core">Core code</a></li>
* <a href="#processing">Webcam processing and rendering</a></li>
* <a href="#puzzle">Webcam puzzle</a></li>
* <a href="#loop">Recording a loop</a></li>
* <a href="#color">Color tracking</a></li>
* <a href="#bg">Background subtraction</a></li>

All the code files for today: 
[Webcam.java](resources/Webcam.java) |
[Webcam.java (10.10)](resources/updated/Webcam.java) |
[WebcamBg.java.java](resources/WebcamBg.java) |
[WebcamColorTracking.java](resources/WebcamColorTracking.java) |
[WebcamProcessing.java](resources/WebcamProcessing.java) |
[WebcamPuzzle.java](resources/WebcamPuzzle.java) |
[WebcamRendering.java](resources/WebcamRendering.java)

<a name="core"></a><h2>Core code</h2>

There's not a standard machinery for processing Webcam in Java. We'll be using 
JavaCV, which is a wrapper over the popular OpenCV image processing library. See 
the <a href="../../software.html">software page</a> for installation details. I've wrapped 
up the details as best I could in a new Webcam class, analogous to DrawingFrame; 
see [Webcam.java](resources/Webcam.java). You should be able to run 
that file on its own and see video. Note that there are two set up parameters: 
"scale", to down-size the image (useful for intensive processing things), and 
"mirror", to flip the image left-right (feels more natural). The constructor 
prints out the native camera size, so you can decide what a good scale factor 
is to yield a sufficiently small image for your machine.

The mechanism for dealing with a webcam can be flakey. It has worked on a variety 
of machines that I've tested; give a yell (Piazza) if it doens't for you. Or 
just use the lab machines in Sudikoff for this bit. One note: when a program 
"freezes", Eclipse has a little red "stop" button in the panel with the console, 
to kill the program without having to resort to your operating system's halt mechanism.

You don't really need to be familiar with the Webcam code. The main things to know 
are that we'll "extend" it (as with DrawingFrame) to define specialized subclasses 
for processing video in different ways. These subclasses will have access to the 
"image" instance variable, which contains the latest grabbed image (but might be 
"null" before anything is grabbed). More simply, there are three methods that we 
can conveniently override when relevant: processImage(), draw(), and 
handleMousePress().

<a name="processing"></a><h2>Webcam processing and rendering</h2>

Since the result of a grab is just our usual BufferedImage, we could apply any 
of our image processing methods to it after it's grabbed and before displaying 
it. Try subclassing and giving something for the processImage() method, as I did 
in [WebcamProcessing.java](resources/WebcamProcessing.java). We can also 
change the rendering by overriding draw(), e.g., by plugging in the mosaic() 
function as in [WebcamRendering.java](resources/WebcamRendering.java).
 
<a name="color"></a><h2>Color tracking</h2>

We can use webcam as a form of user input. For example, given a color, we can 
identify the pixel whose color is closest to that. This could then serve as kind 
of a mouse.

[WebcamColorTracking.java](resources/WebcamColorTracking.java) demonstrates how to 
do such color tracking. Upon getting mouse press, we remember (in "trackColor") 
the color of the pixel at the mouse position. Then when drawing a frame, we call 
a helper method track() to find the Point whose color is closest to that. We draw 
an oval there (after drawing the image). Not particularly robust (in practice, 
we'd assume that the thing we're tracking move smoothly), but kind of works!

<a name="bg"></a><h2>Background subtraction</h2>
 
How can we make a webcam video, shot from the comfort of our room, look like we 
are standing in front of Baker? Filmmakers use the trick of filming in front of 
a green screen, and then replacing the green parts with the desired background 
footage. Since we don't happen to have a green screen here, we'll have to go a 
step further. What we'll do instead is first to take a snapshot of the background 
without us in it. Then for each frame, we'll get rid of that part of the image, 
replacing it with the corresponding part of the desired scenery.

[WebcamBg.java](resources/WebcamBg.java) introduces the method bgSubtract(), 
along with standard code to read in the scenery and capture the "background" upon 
mouse press. The idea is to compare the image pixel's values with those of the 
corresponding background pixel. If they are similar, according to Euclidean 
distance and a specified threshold, then we replace the image pixel with the 
corresponding one from scenery. This works best under controlled lighting and 
if your webcam isn't trying to be too fancy itself by adjusting brightness, etc. 
You'll need to be sure to scale the webcam image down to the size of the background 
scenery (the setup variable "scale").

The same basic idea can be used to detect movement, by subtracting one frame 
from the next.

<a name="puzzle"></a><h2>Webcam puzzle</h2>

Let's get a little discombobulated, making a puzzle from the webcam input, 
[WebcamPuzzle.java](resources/WebcamPuzzle.java). The code is like our earlier 
image-based puzzle (in fact, I started by copying it and bolting on the webcam), 
but we have to be able to create the same layout of rectangles each time we read 
another frame. Thus rather than dividing up the static image into little images 
and swapping them, we keep track of how the pieces should be swapped. Then each 
time we grabe a frame, we slice it up and put the appropriate pieces in the 
appropriate places.

The big change is the introduction of the "pieceR" and "pieceC" arrays. These 
arrays indicate which row and column of the grabbed image should be displayed 
at each row and column in the window. That is, if pieceR[0]=3 and pieceC[0]=4, 
then the pieces[0][0] will take the chunk of the webcam image 3 pieces down and 
4 over. The shuffling method swaps these indices, the piece creation method goes 
through these intermediaries to construct the pieces array, and the swapping upon 
mouse presses acts accordingly.
